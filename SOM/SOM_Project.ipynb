{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Проект 9 — Самоорганизующиеся карты (Self-Organizing Maps, SOM)\n",
        "\n",
        "## Теоретическая часть (Theoretical Background)\n",
        "\n",
        "Самоорганизующиеся карты (Self-Organizing Maps, SOM) — это нейронные сети без учителя, предложенные Тево Кохоненом, для отображения многомерных данных на двумерную решётку.\n",
        "\n",
        "SOM позволяет визуализировать сложные структуры данных и выявлять кластеры.\n",
        "\n",
        "### Основные идеи:\n",
        "\n",
        "1. **Архитектура:**\n",
        "   - Сеть состоит из нейронов, расположенных в сетке (обычно 2D)\n",
        "   - Каждый нейрон имеет весовой вектор той же размерности, что и входные данные\n",
        "\n",
        "2. **Обучение:**\n",
        "   - На каждом шаге выбирается входной вектор, и определяется нейрон-победитель (Best Matching Unit, BMU)\n",
        "   - Веса нейрона-победителя и его соседей обновляются\n",
        "   - Функция соседства уменьшается с расстоянием\n",
        "\n",
        "3. **Свойства:**\n",
        "   - Сохраняет топологию данных\n",
        "   - Уменьшает размерность\n",
        "   - Обеспечивает визуальное представление кластеров\n",
        "\n",
        "4. **Типичные применения:**\n",
        "   - Кластеризация данных\n",
        "   - Визуализация многомерных признаков\n",
        "   - Сегментация клиентов, изображений или биологических данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Импорт библиотек (Import Libraries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import kagglehub\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка для отображения графиков\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задача 1: Подготовка данных (Data Preparation)\n",
        "\n",
        "Загрузим и подготовим данные Customer Personality Analysis Dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка датасета Customer Personality Analysis с Kaggle\n",
        "print(\"Загрузка датасета с Kaggle...\")\n",
        "try:\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"imakash3011/customer-personality-analysis\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "    \n",
        "    # Ищем файл с данными в загруженной директории\n",
        "    # Обычно файл называется marketing_campaign.csv\n",
        "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
        "    \n",
        "    if csv_files:\n",
        "        # Используем первый найденный CSV файл\n",
        "        csv_file = os.path.join(path, csv_files[0])\n",
        "        print(f\"Найден файл: {csv_file}\")\n",
        "        \n",
        "        # Пробуем загрузить с разными разделителями\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file, sep='\\t')\n",
        "            print(\"Данные загружены с разделителем табуляции\")\n",
        "        except:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file, sep=',')\n",
        "                print(\"Данные загружены с разделителем запятой\")\n",
        "            except:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                print(\"Данные загружены с автоматическим определением разделителя\")\n",
        "    else:\n",
        "        # Если CSV файл не найден, ищем в поддиректориях\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv'):\n",
        "                    csv_file = os.path.join(root, file)\n",
        "                    print(f\"Найден файл в поддиректории: {csv_file}\")\n",
        "                    try:\n",
        "                        df = pd.read_csv(csv_file, sep='\\t')\n",
        "                        print(\"Данные загружены с разделителем табуляции\")\n",
        "                        break\n",
        "                    except:\n",
        "                        try:\n",
        "                            df = pd.read_csv(csv_file, sep=',')\n",
        "                            print(\"Данные загружены с разделителем запятой\")\n",
        "                            break\n",
        "                        except:\n",
        "                            df = pd.read_csv(csv_file)\n",
        "                            print(\"Данные загружены с автоматическим определением разделителя\")\n",
        "                            break\n",
        "            else:\n",
        "                continue\n",
        "            break\n",
        "        else:\n",
        "            raise FileNotFoundError(\"CSV файл не найден в загруженном датасете\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке с Kaggle: {e}\")\n",
        "    print(\"Попытка загрузить из локального файла...\")\n",
        "    try:\n",
        "        df = pd.read_csv('marketing_campaign.csv', sep='\\t')\n",
        "        print(\"Данные загружены из локального файла\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Локальный файл не найден. Используются синтетические данные для демонстрации...\")\n",
        "        # Создаём синтетические данные для демонстрации\n",
        "        np.random.seed(42)\n",
        "        n_samples = 2000\n",
        "        df = pd.DataFrame({\n",
        "            'Year_Birth': np.random.randint(1940, 2000, n_samples),\n",
        "            'Education': np.random.choice(['Graduation', 'PhD', 'Master', 'Basic', '2n Cycle'], n_samples),\n",
        "            'Marital_Status': np.random.choice(['Single', 'Together', 'Married', 'Divorced', 'Widow'], n_samples),\n",
        "            'Income': np.random.normal(50000, 20000, n_samples),\n",
        "            'Kidhome': np.random.choice([0, 1, 2], n_samples),\n",
        "            'Teenhome': np.random.choice([0, 1, 2], n_samples),\n",
        "            'Recency': np.random.randint(0, 100, n_samples),\n",
        "            'MntWines': np.random.poisson(300, n_samples),\n",
        "            'MntFruits': np.random.poisson(50, n_samples),\n",
        "            'MntMeatProducts': np.random.poisson(200, n_samples),\n",
        "            'MntFishProducts': np.random.poisson(50, n_samples),\n",
        "            'MntSweetProducts': np.random.poisson(50, n_samples),\n",
        "            'MntGoldProds': np.random.poisson(50, n_samples),\n",
        "            'NumDealsPurchases': np.random.poisson(2, n_samples),\n",
        "            'NumWebPurchases': np.random.poisson(5, n_samples),\n",
        "            'NumCatalogPurchases': np.random.poisson(3, n_samples),\n",
        "            'NumStorePurchases': np.random.poisson(6, n_samples),\n",
        "            'NumWebVisitsMonth': np.random.poisson(5, n_samples),\n",
        "            'AcceptedCmp1': np.random.choice([0, 1], n_samples),\n",
        "            'AcceptedCmp2': np.random.choice([0, 1], n_samples),\n",
        "            'AcceptedCmp3': np.random.choice([0, 1], n_samples),\n",
        "            'AcceptedCmp4': np.random.choice([0, 1], n_samples),\n",
        "            'AcceptedCmp5': np.random.choice([0, 1], n_samples),\n",
        "            'Response': np.random.choice([0, 1], n_samples),\n",
        "            'Complain': np.random.choice([0, 1], n_samples)\n",
        "        })\n",
        "        df['Income'] = np.abs(df['Income'])  # Убедимся, что доход положительный\n",
        "        print(\"Синтетические данные созданы\")\n",
        "\n",
        "print(f\"\\nРазмер датасета: {df.shape}\")\n",
        "print(f\"\\nПервые строки:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Информация о данных\n",
        "print(\"Информация о датасете:\")\n",
        "print(df.info())\n",
        "print(\"\\nОписательная статистика:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обработка пропущенных значений\n",
        "print(\"Пропущенные значения:\")\n",
        "print(df.isnull().sum().sum())\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    df = df.dropna()\n",
        "    print(f\"Удалены строки с пропущенными значениями. Новый размер: {df.shape}\")\n",
        "\n",
        "# Выбор ключевых признаков для анализа\n",
        "# Выберем числовые признаки, связанные с поведением клиентов\n",
        "feature_columns = [\n",
        "    'Year_Birth',\n",
        "    'Income',\n",
        "    'Recency',\n",
        "    'MntWines',\n",
        "    'MntFruits',\n",
        "    'MntMeatProducts',\n",
        "    'MntFishProducts',\n",
        "    'MntSweetProducts',\n",
        "    'MntGoldProds',\n",
        "    'NumDealsPurchases',\n",
        "    'NumWebPurchases',\n",
        "    'NumCatalogPurchases',\n",
        "    'NumStorePurchases',\n",
        "    'NumWebVisitsMonth'\n",
        "]\n",
        "\n",
        "# Проверяем наличие всех колонок\n",
        "available_features = [col for col in feature_columns if col in df.columns]\n",
        "print(f\"\\nИспользуемые признаки: {available_features}\")\n",
        "\n",
        "# Создаём датасет с выбранными признаками\n",
        "data = df[available_features].copy()\n",
        "\n",
        "# Вычисляем возраст из года рождения\n",
        "if 'Year_Birth' in data.columns:\n",
        "    data['Age'] = 2024 - data['Year_Birth']\n",
        "    data = data.drop('Year_Birth', axis=1)\n",
        "\n",
        "print(f\"\\nФинальный размер данных: {data.shape}\")\n",
        "print(f\"\\nПризнаки: {list(data.columns)}\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Нормализация данных\n",
        "# Импортируем необходимые библиотеки (на случай, если ячейка с импортами не была выполнена)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n",
        "\n",
        "print(\"Данные нормализованы (StandardScaler)\")\n",
        "print(f\"Среднее: {data_scaled.mean().mean():.6f}\")\n",
        "print(f\"Стандартное отклонение: {data_scaled.std().mean():.6f}\")\n",
        "print(\"\\nПервые строки нормализованных данных:\")\n",
        "data_scaled.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задача 2: Реализация SOM (SOM Implementation)\n",
        "\n",
        "Реализуем класс Self-Organizing Map с сеткой нейронов 10×10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfOrganizingMap:\n",
        "    \"\"\"\n",
        "    Реализация самоорганизующейся карты Кохонена (SOM)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, grid_size=(10, 10), input_dim=None, learning_rate=0.5, \n",
        "                 radius=None, random_seed=42):\n",
        "        \"\"\"\n",
        "        Параметры:\n",
        "        - grid_size: размер сетки (высота, ширина)\n",
        "        - input_dim: размерность входных данных\n",
        "        - learning_rate: начальная скорость обучения\n",
        "        - radius: начальный радиус соседства (если None, вычисляется автоматически)\n",
        "        - random_seed: seed для воспроизводимости\n",
        "        \"\"\"\n",
        "        self.grid_size = grid_size\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.random_seed = random_seed\n",
        "        \n",
        "        if radius is None:\n",
        "            # Начальный радиус обычно равен половине размера сетки\n",
        "            self.radius = max(grid_size) / 2\n",
        "        else:\n",
        "            self.radius = radius\n",
        "        \n",
        "        # Инициализация весов случайными значениями\n",
        "        np.random.seed(random_seed)\n",
        "        self.weights = np.random.randn(grid_size[0], grid_size[1], input_dim)\n",
        "        \n",
        "        # Создаём координаты нейронов в сетке\n",
        "        self.coords = np.array([[i, j] for i in range(grid_size[0]) \n",
        "                                for j in range(grid_size[1])])\n",
        "        self.coords = self.coords.reshape(grid_size[0], grid_size[1], 2)\n",
        "        \n",
        "    def find_bmu(self, input_vector):\n",
        "        \"\"\"\n",
        "        Находит Best Matching Unit (BMU) - нейрон-победитель\n",
        "        для данного входного вектора\n",
        "        \"\"\"\n",
        "        # Вычисляем евклидово расстояние от входного вектора до всех нейронов\n",
        "        distances = np.sum((self.weights - input_vector) ** 2, axis=2)\n",
        "        # Находим индекс минимального расстояния\n",
        "        bmu_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
        "        return bmu_idx\n",
        "    \n",
        "    def neighborhood_function(self, distance, radius):\n",
        "        \"\"\"\n",
        "        Функция соседства (гауссова)\n",
        "        \"\"\"\n",
        "        return np.exp(-(distance ** 2) / (2 * (radius ** 2)))\n",
        "    \n",
        "    def update_weights(self, input_vector, bmu_idx, learning_rate, radius):\n",
        "        \"\"\"\n",
        "        Обновляет веса нейронов на основе BMU и функции соседства\n",
        "        \"\"\"\n",
        "        # Вычисляем расстояния от BMU до всех нейронов в сетке\n",
        "        bmu_coords = np.array(bmu_idx)\n",
        "        distances = np.sum((self.coords - bmu_coords) ** 2, axis=2)\n",
        "        distances = np.sqrt(distances)\n",
        "        \n",
        "        # Вычисляем функцию соседства\n",
        "        neighborhood = self.neighborhood_function(distances, radius)\n",
        "        \n",
        "        # Обновляем веса\n",
        "        for i in range(self.grid_size[0]):\n",
        "            for j in range(self.grid_size[1]):\n",
        "                influence = neighborhood[i, j] * learning_rate\n",
        "                self.weights[i, j] += influence * (input_vector - self.weights[i, j])\n",
        "    \n",
        "    def train(self, data, n_iterations=1000, verbose=True):\n",
        "        \"\"\"\n",
        "        Обучает SOM на данных\n",
        "        \n",
        "        Параметры:\n",
        "        - data: нормализованные данные (numpy array или pandas DataFrame)\n",
        "        - n_iterations: количество итераций обучения\n",
        "        - verbose: выводить ли прогресс\n",
        "        \"\"\"\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            data = data.values\n",
        "        \n",
        "        n_samples = len(data)\n",
        "        \n",
        "        # Параметры для адаптивного обучения\n",
        "        initial_lr = self.learning_rate\n",
        "        initial_radius = self.radius\n",
        "        \n",
        "        for iteration in range(n_iterations):\n",
        "            # Выбираем случайный образец\n",
        "            sample_idx = np.random.randint(0, n_samples)\n",
        "            sample = data[sample_idx]\n",
        "            \n",
        "            # Находим BMU\n",
        "            bmu_idx = self.find_bmu(sample)\n",
        "            \n",
        "            # Адаптивно уменьшаем скорость обучения и радиус\n",
        "            current_lr = initial_lr * np.exp(-iteration / n_iterations)\n",
        "            current_radius = initial_radius * np.exp(-iteration / n_iterations)\n",
        "            \n",
        "            # Обновляем веса\n",
        "            self.update_weights(sample, bmu_idx, current_lr, current_radius)\n",
        "            \n",
        "            if verbose and (iteration + 1) % (n_iterations // 10) == 0:\n",
        "                print(f\"Итерация {iteration + 1}/{n_iterations} завершена\")\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Предсказывает BMU для каждого образца данных\n",
        "        \"\"\"\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            data = data.values\n",
        "        \n",
        "        predictions = []\n",
        "        for sample in data:\n",
        "            bmu_idx = self.find_bmu(sample)\n",
        "            predictions.append(bmu_idx)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def get_u_matrix(self):\n",
        "        \"\"\"\n",
        "        Вычисляет U-Matrix (Unified Distance Matrix) для визуализации\n",
        "        U-Matrix показывает средние расстояния между соседними нейронами\n",
        "        \"\"\"\n",
        "        u_matrix = np.zeros(self.grid_size)\n",
        "        \n",
        "        for i in range(self.grid_size[0]):\n",
        "            for j in range(self.grid_size[1]):\n",
        "                neighbors = []\n",
        "                \n",
        "                # Проверяем соседей (вверх, вниз, влево, вправо)\n",
        "                if i > 0:\n",
        "                    neighbors.append(self.weights[i-1, j])\n",
        "                if i < self.grid_size[0] - 1:\n",
        "                    neighbors.append(self.weights[i+1, j])\n",
        "                if j > 0:\n",
        "                    neighbors.append(self.weights[i, j-1])\n",
        "                if j < self.grid_size[1] - 1:\n",
        "                    neighbors.append(self.weights[i, j+1])\n",
        "                \n",
        "                if neighbors:\n",
        "                    # Вычисляем среднее расстояние до соседей\n",
        "                    distances = [np.linalg.norm(self.weights[i, j] - neighbor) \n",
        "                                for neighbor in neighbors]\n",
        "                    u_matrix[i, j] = np.mean(distances)\n",
        "        \n",
        "        return u_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создаём и обучаем SOM\n",
        "print(\"Создание SOM с сеткой 10x10...\")\n",
        "som = SelfOrganizingMap(\n",
        "    grid_size=(10, 10),\n",
        "    input_dim=data_scaled.shape[1],\n",
        "    learning_rate=0.5,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "print(\"Начало обучения...\")\n",
        "som.train(data_scaled, n_iterations=1000, verbose=True)\n",
        "print(\"Обучение завершено!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вычисляем U-Matrix\n",
        "u_matrix = som.get_u_matrix()\n",
        "\n",
        "# Визуализация U-Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(u_matrix, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar(label='Среднее расстояние до соседей')\n",
        "plt.title('U-Matrix (Unified Distance Matrix)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Ширина сетки')\n",
        "plt.ylabel('Высота сетки')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Интерпретация: тёмные области = кластеры, светлые = границы между кластерами\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Альтернативная визуализация U-Matrix с контурными линиями\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Тепловая карта\n",
        "im1 = ax1.imshow(u_matrix, cmap='viridis', interpolation='nearest')\n",
        "ax1.set_title('U-Matrix (тепловая карта)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Ширина сетки')\n",
        "ax1.set_ylabel('Высота сетки')\n",
        "plt.colorbar(im1, ax=ax1, label='Среднее расстояние')\n",
        "\n",
        "# Контурная карта\n",
        "contour = ax2.contour(u_matrix, levels=15, cmap='viridis')\n",
        "ax2.clabel(contour, inline=True, fontsize=8)\n",
        "ax2.set_title('U-Matrix (контурная карта)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Ширина сетки')\n",
        "ax2.set_ylabel('Высота сетки')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация распределения данных по карте\n",
        "predictions = som.predict(data_scaled)\n",
        "\n",
        "# Создаём карту частоты активации нейронов\n",
        "activation_map = np.zeros(som.grid_size)\n",
        "for pred in predictions:\n",
        "    activation_map[pred[0], pred[1]] += 1\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(activation_map, cmap='YlOrRd', interpolation='nearest')\n",
        "plt.colorbar(label='Количество активированных образцов')\n",
        "plt.title('Карта активации нейронов', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Ширина сетки')\n",
        "plt.ylabel('Высота сетки')\n",
        "\n",
        "# Добавляем аннотации с количеством\n",
        "for i in range(som.grid_size[0]):\n",
        "    for j in range(som.grid_size[1]):\n",
        "        if activation_map[i, j] > 0:\n",
        "            plt.text(j, i, int(activation_map[i, j]), \n",
        "                    ha='center', va='center', \n",
        "                    color='white' if activation_map[i, j] > activation_map.max()/2 else 'black',\n",
        "                    fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задача 4: Определение кластеров и интерпретация результатов\n",
        "\n",
        "Определим кластеры клиентов на основе SOM и проанализируем их характеристики.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Используем K-Means на весах SOM для определения кластеров\n",
        "# Это позволяет автоматически определить количество кластеров\n",
        "\n",
        "# Получаем веса всех нейронов\n",
        "neuron_weights = som.weights.reshape(-1, som.input_dim)\n",
        "\n",
        "# Определяем оптимальное количество кластеров с помощью метода локтя\n",
        "inertias = []\n",
        "K_range = range(2, 11)\n",
        "for k in K_range:\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_temp.fit(neuron_weights)\n",
        "    inertias.append(kmeans_temp.inertia_)\n",
        "\n",
        "# Визуализация метода локтя\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertias, 'bo-')\n",
        "plt.xlabel('Количество кластеров (K)')\n",
        "plt.ylabel('Инерция (Inertia)')\n",
        "plt.title('Метод локтя для определения оптимального K', fontsize=14, fontweight='bold')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Выбираем оптимальное K (обычно 4-6 для такого датасета)\n",
        "optimal_k = 5\n",
        "print(f\"Выбрано количество кластеров: {optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Применяем K-Means к весам нейронов SOM\n",
        "kmeans_som = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "neuron_clusters = kmeans_som.fit_predict(neuron_weights)\n",
        "neuron_clusters = neuron_clusters.reshape(som.grid_size)\n",
        "\n",
        "# Визуализация кластеров на карте\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(neuron_clusters, cmap='tab10', interpolation='nearest')\n",
        "plt.colorbar(label='Кластер')\n",
        "plt.title(f'Кластеризация SOM (K={optimal_k})', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Ширина сетки')\n",
        "plt.ylabel('Высота сетки')\n",
        "\n",
        "# Добавляем границы кластеров\n",
        "for i in range(som.grid_size[0]):\n",
        "    for j in range(som.grid_size[1]):\n",
        "        # Проверяем соседей для определения границ\n",
        "        if i < som.grid_size[0] - 1 and neuron_clusters[i, j] != neuron_clusters[i+1, j]:\n",
        "            plt.axhline(i + 0.5, color='black', linewidth=0.5)\n",
        "        if j < som.grid_size[1] - 1 and neuron_clusters[i, j] != neuron_clusters[i, j+1]:\n",
        "            plt.axvline(j + 0.5, color='black', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Назначаем кластеры каждому образцу данных\n",
        "data_predictions = som.predict(data_scaled)\n",
        "sample_clusters = np.array([neuron_clusters[pred[0], pred[1]] for pred in data_predictions])\n",
        "\n",
        "# Добавляем информацию о кластерах к исходным данным\n",
        "df_with_clusters = df.copy()\n",
        "df_with_clusters['SOM_Cluster'] = sample_clusters\n",
        "\n",
        "print(\"Распределение образцов по кластерам:\")\n",
        "print(df_with_clusters['SOM_Cluster'].value_counts().sort_index())\n",
        "print(f\"\\nВсего кластеров: {len(df_with_clusters['SOM_Cluster'].unique())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Анализ характеристик кластеров\n",
        "print(\"=\" * 80)\n",
        "print(\"АНАЛИЗ ХАРАКТЕРИСТИК КЛАСТЕРОВ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cluster_analysis = df_with_clusters.groupby('SOM_Cluster')[available_features].mean()\n",
        "\n",
        "# Добавляем размер кластера\n",
        "cluster_sizes = df_with_clusters['SOM_Cluster'].value_counts().sort_index()\n",
        "cluster_analysis['Cluster_Size'] = cluster_sizes.values\n",
        "\n",
        "print(\"\\nСредние значения признаков по кластерам:\")\n",
        "print(cluster_analysis.round(2))\n",
        "\n",
        "# Визуализация характеристик кластеров\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Размер кластеров\n",
        "axes[0, 0].bar(cluster_sizes.index, cluster_sizes.values, color='steelblue')\n",
        "axes[0, 0].set_title('Размер кластеров', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Кластер')\n",
        "axes[0, 0].set_ylabel('Количество клиентов')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Доход по кластерам\n",
        "if 'Income' in available_features:\n",
        "    income_by_cluster = df_with_clusters.groupby('SOM_Cluster')['Income'].mean()\n",
        "    axes[0, 1].bar(income_by_cluster.index, income_by_cluster.values, color='coral')\n",
        "    axes[0, 1].set_title('Средний доход по кластерам', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Кластер')\n",
        "    axes[0, 1].set_ylabel('Средний доход')\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. Расходы на вино по кластерам\n",
        "if 'MntWines' in available_features:\n",
        "    wine_by_cluster = df_with_clusters.groupby('SOM_Cluster')['MntWines'].mean()\n",
        "    axes[1, 0].bar(wine_by_cluster.index, wine_by_cluster.values, color='green')\n",
        "    axes[1, 0].set_title('Средние расходы на вино по кластерам', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Кластер')\n",
        "    axes[1, 0].set_ylabel('Средние расходы')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Количество покупок в магазине\n",
        "if 'NumStorePurchases' in available_features:\n",
        "    store_by_cluster = df_with_clusters.groupby('SOM_Cluster')['NumStorePurchases'].mean()\n",
        "    axes[1, 1].bar(store_by_cluster.index, store_by_cluster.values, color='purple')\n",
        "    axes[1, 1].set_title('Среднее количество покупок в магазине', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Кластер')\n",
        "    axes[1, 1].set_ylabel('Среднее количество покупок')\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Тепловая карта характеристик кластеров\n",
        "if 'Income' in available_features:\n",
        "    # Выбираем ключевые признаки для визуализации\n",
        "    key_features = ['Income', 'MntWines', 'MntMeatProducts', 'NumStorePurchases', \n",
        "                    'NumWebPurchases', 'NumCatalogPurchases', 'Recency']\n",
        "    key_features = [f for f in key_features if f in available_features]\n",
        "    \n",
        "    cluster_heatmap = df_with_clusters.groupby('SOM_Cluster')[key_features].mean()\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(cluster_heatmap.T, annot=True, fmt='.1f', cmap='YlOrRd', \n",
        "                cbar_kws={'label': 'Среднее значение'})\n",
        "    plt.title('Тепловая карта характеристик кластеров', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Кластер')\n",
        "    plt.ylabel('Признак')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Интерпретация кластеров\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ИНТЕРПРЕТАЦИЯ КЛАСТЕРОВ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for cluster_id in sorted(df_with_clusters['SOM_Cluster'].unique()):\n",
        "    cluster_data = df_with_clusters[df_with_clusters['SOM_Cluster'] == cluster_id]\n",
        "    print(f\"\\n--- Кластер {cluster_id} ---\")\n",
        "    print(f\"Размер: {len(cluster_data)} клиентов ({len(cluster_data)/len(df_with_clusters)*100:.1f}%)\")\n",
        "    \n",
        "    if 'Income' in available_features:\n",
        "        print(f\"Средний доход: ${cluster_data['Income'].mean():.2f}\")\n",
        "    if 'Age' in df_with_clusters.columns:\n",
        "        print(f\"Средний возраст: {cluster_data['Age'].mean():.1f} лет\")\n",
        "    if 'MntWines' in available_features:\n",
        "        print(f\"Средние расходы на вино: ${cluster_data['MntWines'].mean():.2f}\")\n",
        "    if 'NumStorePurchases' in available_features:\n",
        "        print(f\"Среднее количество покупок в магазине: {cluster_data['NumStorePurchases'].mean():.2f}\")\n",
        "    if 'Recency' in available_features:\n",
        "        print(f\"Средняя давность последней покупки: {cluster_data['Recency'].mean():.1f} дней\")\n",
        "    \n",
        "    # Определяем тип кластера\n",
        "    if 'Income' in available_features and 'MntWines' in available_features:\n",
        "        avg_income = cluster_data['Income'].mean()\n",
        "        avg_wine = cluster_data['MntWines'].mean()\n",
        "        \n",
        "        if avg_income > df_with_clusters['Income'].quantile(0.75):\n",
        "            if avg_wine > df_with_clusters['MntWines'].quantile(0.75):\n",
        "                cluster_type = \"Премиум клиенты (высокий доход, высокие расходы)\"\n",
        "            else:\n",
        "                cluster_type = \"Богатые, но экономные\"\n",
        "        elif avg_income < df_with_clusters['Income'].quantile(0.25):\n",
        "            cluster_type = \"Бюджетные клиенты\"\n",
        "        else:\n",
        "            cluster_type = \"Средний сегмент\"\n",
        "        \n",
        "        print(f\"Тип: {cluster_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задача 5: Сравнение с K-Means\n",
        "\n",
        "Сравним результаты кластеризации SOM с методом K-Means.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Применяем K-Means напрямую к данным\n",
        "kmeans_direct = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans_clusters = kmeans_direct.fit_predict(data_scaled)\n",
        "\n",
        "df_with_clusters['KMeans_Cluster'] = kmeans_clusters\n",
        "\n",
        "print(\"Распределение по кластерам K-Means:\")\n",
        "print(df_with_clusters['KMeans_Cluster'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация сравнения кластеров\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Используем PCA для визуализации в 2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "data_2d = pca.fit_transform(data_scaled)\n",
        "\n",
        "# SOM кластеры\n",
        "scatter1 = axes[0].scatter(data_2d[:, 0], data_2d[:, 1], \n",
        "                          c=sample_clusters, cmap='tab10', alpha=0.6, s=20)\n",
        "axes[0].set_title(f'SOM кластеризация (K={optimal_k})', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Кластер')\n",
        "\n",
        "# K-Means кластеры\n",
        "scatter2 = axes[1].scatter(data_2d[:, 0], data_2d[:, 1], \n",
        "                          c=kmeans_clusters, cmap='tab10', alpha=0.6, s=20)\n",
        "axes[1].set_title(f'K-Means кластеризация (K={optimal_k})', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Кластер')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Матрица сопряжённости для сравнения кластеров\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "# Вычисляем метрики сходства\n",
        "ari_score = adjusted_rand_score(sample_clusters, kmeans_clusters)\n",
        "nmi_score = normalized_mutual_info_score(sample_clusters, kmeans_clusters)\n",
        "\n",
        "print(\"Метрики сравнения кластеризаций:\")\n",
        "print(f\"Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
        "print(f\"Normalized Mutual Information (NMI): {nmi_score:.4f}\")\n",
        "print(\"\\nИнтерпретация:\")\n",
        "print(\"- ARI = 1.0: идентичные кластеризации\")\n",
        "print(\"- ARI = 0.0: случайное совпадение\")\n",
        "print(\"- NMI = 1.0: полная взаимная информация\")\n",
        "print(\"- NMI = 0.0: нет взаимной информации\")\n",
        "\n",
        "# Матрица сопряжённости\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(sample_clusters, kmeans_clusters)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=[f'K-Means {i}' for i in range(optimal_k)],\n",
        "            yticklabels=[f'SOM {i}' for i in range(optimal_k)])\n",
        "plt.title('Матрица сопряжённости: SOM vs K-Means', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('K-Means кластеры')\n",
        "plt.ylabel('SOM кластеры')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Выводы (Conclusions)\n",
        "\n",
        "### Основные результаты:\n",
        "\n",
        "1. **SOM успешно обучена** на данных о клиентах с сеткой 10×10 нейронов\n",
        "2. **U-Matrix визуализация** показала структуру кластеров в данных\n",
        "3. **Выявлены кластеры клиентов** с различными характеристиками поведения\n",
        "4. **Сравнение с K-Means** показало различия и сходства в подходах к кластеризации\n",
        "\n",
        "### Преимущества SOM:\n",
        "\n",
        "- **Топологическое сохранение**: похожие данные располагаются близко на карте\n",
        "- **Визуализация**: U-Matrix позволяет визуально идентифицировать кластеры\n",
        "- **Интерпретируемость**: можно анализировать характеристики каждого региона карты\n",
        "\n",
        "### Отличия от K-Means:\n",
        "\n",
        "- SOM сохраняет топологию данных, K-Means - нет\n",
        "- SOM предоставляет визуальное представление структуры данных\n",
        "- K-Means быстрее, но менее информативен для визуализации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
